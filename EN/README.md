# Summary of projects

Link | Direction | Purpose | Practiced Skills | Libraries used
------------- |------------- |---------------- | ---------------- | ----------------------
[Used car price forecasting](https://github.com/AndreiShagin/AndreiShagin_Projects/blob/main/EN/ML_car_price_predict_regression/ML_car_price_predict_regression.ipynb) | Machine learning (gradient boosting, regression) | Train the model to determine the market value of a car. | Pre-processing done (duplicates removed, missing values replaced). Models trained and predicted: CatBoost, LightGBM, LinearRegression, Random Forrest with default parameters and using sets of hyperparameters. The best model was selected based on the results of the RMSE 1525 metric and training time. | `Pandas`, `NumPy`, `Sklearn`, `CatBoost`, `GridSearchCV`, `LightGBM`, `CatBoost`, `Seaborn`, `OrdinalEncoder`, `OHE`
[Toxic Comments Classification](https://github.com/AndreiShagin/AndreiShagin_Projects/blob/main/EN/ML_analysis_toxic_comments_classification/ML_NLP_toxic_comments.ipynb) | NLP, Machine learning | Classify comments to send negative ones for moderation. Determine and train a model with the value of the quality metric F1 not less than 0.75. | On the validation set, the CatBoostRegressor model showed the result F1 0.764, on the test set 0.756. During execution, text was cleaned using regular expressions, lemmatization with POS tag, three models were trained, and hyperparameters were selected | `Pandas`, `NumPy`, `Sklearn`, `CatBoostRegressor`, `NLTK`, `LogisticRegression`, `Matplotlib`, `Seaborn`, `WordNetLemmatizer`, `DecisionTreeClassifier`,`tf-idf`
[Taxi order prediction](https://github.com/AndreiShagin/AndreiShagin_Projects/blob/main/EN/ML_Taxi_orders_predict_time_series/Taxi_orders_predict_ML_time_series.ipynb) | Machine learning (time series, regression, predictions) | Train a model to predict a taxi orders numbers for the next hour | An analysis was carried out to determine trends by hours during the day and by days. Models used: LogisticRegression , RandomForestRegressor, CatBoostRegressor with default parameters and sets of hyperparameters. The best model was selected according to the results of the RMSE metric. | `Pandas`, `NumPy`, `Sklearn`, `CatBoostRegressor`, `GridSearchCV`, `LogisticRegression`, `Matplotlib`, `Seaborn`, `RandomForestRegressor`, `TimeSeriesSplit`
[Defining Fake and Real Disaster Tweets, Kaggle Project](https://github.com/AndreiShagin/AndreiShagin_Projects/blob/main/EN/Kaggle_ML_NLP_Disaster_Tweets/Kaggle_NLP_with_Disaster_Tweets.ipynb) | Machine learning, NLP | Train a model to predict whether a tweet is fake or real, target metric F1 | On the training set, the LogicRegression model showed the result F1 0.76, on the test set 0.79. During execution, text was cleaned using regular expressions, lemmatization with POS tag, 4 models were trained, and hyperparameters were selected. | `Pandas`, `NumPy`, `Sklearn`, `CatBoostRegressor`, `NLTK`, `LogisticRegression`, `Matplotlib`, `Seaborn`, `WordNetLemmatizer`, `LightGBM`,`TfidfVectorizer` , `DecisionTree`
[House Prices Prediction, Kaggle Project](https://github.com/AndreiShagin/AndreiShagin_Projects/blob/main/EN/Kaggle_ML_house_prices_predict/Kaggle_house_prices_predict.ipynb) | Machine learning, regression | Train a model to predict the final price of each home | On the test CatBoost get 0.12 RMSE. Top 13% solution. | `Pandas`, `NumPy`, `Sklearn`, `CatBoostRegressor`, `LGBMRegressor`, `DecisionTreeRegressor`, `Matplotlib`, `Seaborn`, `StackingRegressor`, `LinearRegression`,`GradientBoostingRegressor` , `RandomForestRegressor`, `GridSearchCV`, `RandomForestRegressor`
